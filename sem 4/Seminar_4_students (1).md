
---

**1. Экспорт простой модели PyTorch в ONNX**  
- Возьмите любую «игрушечную» сверточную сеть или MLP, реализованную в PyTorch (обучение не обязательно — достаточно задания архитектуры).  
- Подготовьте Python-скрипт, который:
  1. Создаёт (или загружает) модель.
  2. Переводит модель в режим `eval()`.
  3. Экспортирует модель в формат ONNX с помощью `torch.onnx.export`.  
- Проверьте корректность полученного файла `.onnx`, используя:
  - `onnx.checker.check_model` для базовой валидации,  
  - или `onnxruntime.InferenceSession` для проверки инференса.

---

**2. Сборка TensorRT engine в режиме FP32 и FP16**  
- Напишите Python-скрипт, в котором:
  1. Импортируется модуль TensorRT (`import tensorrt as trt`).
  2. Создаётся `Builder`, `Network` (с флагом `EXPLICIT_BATCH`) и `OnnxParser`.
  3. Загружается ранее экспортированный `.onnx`-файл и строится engine в режиме FP32.
  4. Аналогично, с установкой флага `trt.BuilderFlag.FP16`, создаётся engine в режиме FP16.
- Сериализуйте оба варианта engine в файлы (например, `model_fp32.plan` и `model_fp16.plan`) и зафиксируйте их размеры (в байтах или мегабайтах).

---

**3. Замер производительности**  
- Для каждого engine (FP32 и FP16) создайте функцию инференса на Python, которая:
  1. Загружает engine с использованием `trt.Runtime` и метода `deserialize_cuda_engine`.
  2. Создаёт `ExecutionContext` для выполнения инференса.
  3. Формирует тестовый входной массив (например, с помощью `np.random.randn(...)` соответствующей формы).
  4. Выполняет несколько сотен циклических прогонов инференса и замеряет среднее время (используйте `time.perf_counter()` для высокой точности).
- Сравните производительность:
  - Измерьте пропускную способность (число инференсов в секунду) или усреднённую задержку (в миллисекундах) для каждого режима.
  - Определите, какой движок (FP32 или FP16) обеспечивает лучший результат.

---

**4. Эксперимент с INT8 (необязательно)**  
- Если остаётся время, добавьте этап калибровки для INT8:
  1. Реализуйте небольшой класс-калибратор, унаследованный от `trt.IInt8EntropyCalibrator2` (или `trt.IInt8MinMaxCalibrator`), для проведения калибровки.
  2. Соберите engine в режиме INT8, установив флаг `trt.BuilderFlag.INT8` и задав калибратор через `config.int8_calibrator`.
  3. Сериализуйте INT8 engine (например, в файл `model_int8.plan`).
- Сравните полученные результаты:
  - Проверьте корректность работы (выходные данные, точность).
  - Замерьте скорость инференса и сравните с режимами FP32 и FP16.
- Зафиксируйте, насколько изменились метрики производительности и (при возможности) точности.

---

**5. Мини-бенчмарк vs PyTorch**  
- Замерьте время инференса исходной модели на PyTorch (например, вызовом `model(input_torch)`) на GPU (без дополнительных оптимизаций).  
- Сравните результаты с измеренными показателями для TensorRT engine в режимах FP32, FP16 (и, если выполнено, INT8).  
- Определите реальный прирост производительности при использовании TensorRT по сравнению с прямым инференсом в PyTorch.

---
